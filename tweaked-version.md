# Technological and scientific objectives *

(1.1) During the claim period (January 2025 to December 2025), the technological advance sought was to develop a new, software-enabled method for structuring and developing deep, multi-step qualitative NLP analyses using Large Language Models (LLMs). In particular, this method was intended to make it possible to execute complex, interdependent analytical prompt chains comprising 10 or more dependent steps for verbal reasoning. If successful, this would represent an increase in overall knowledge and capability in the field and, therefore, aligns with Paragraph 9(b) of the DSIT Guidelines.

(2.1) To achieve the advance, and as part of the advance, the team of competent professionals needed to engineer a reporting framework that established coherent, explainable, and sufficiently stable analytical reporting outputs. Additionally, achieving the advance was dependent on engineering a new state-management layer, referred to as the ‘Issue Register’, to decouple analytical reasoning from non-deterministic prose generation. The team planned to achieve this by developing a set of state-machine controls to manage the stochastic nature of large-scale reasoning models, ensuring consistency and clarity in how different parts worked together. 

(2.2) If successful, the R&D project would demonstrate that deep prompt chains are a reliable engineering technique for long-chain analytical reporting, especially as they could turn source artefacts into structured, usable written analyses through multiple interdependent reasoning steps.

(2.3) In addition to that mentioned above, the success of the R&D project hinged on the ability to repeatedly execute these 10 or more-step verbal-reasoning chains with an end-to-end failure rate of less than 5 %. To enable manageable levels of iteration, the team sought to develop a capability that allowed small changes to a single step to be made without causing non-local regressions, at most 200 % of the time it takes to iterate a prompt of equivalent complexity in isolation.

(3.1) Further technological objectives included engineering high-precision regression-detection capabilities. In particular, the team aimed to detect 99.9 % of major regressions (defined as changes that reduce end-to-end success below the 95 % requirement) while also detecting 95 % of minor regressions that represent measurable reductions in specific quality dimensions. Achieving these targets required the development of Large Language Model (LLM)-based meta-tools to accelerate iteration, detect quality regressions, and monitor progress towards domain-specific improvement targets.

(3.2) Other competent professionals working in the field would recognise this as a technological advance because it required the creation of a practical engineering pattern where reasoning was structured, audited, and iterated before final narrative assembly; this represented an increase in capability beyond ‘routine’ prompt-chaining as it integrated formal regression controls and explicit trace links back to source inputs. In doing so, this ensured that analytical-reporting chains could be updated within a predictable validation effort rather than having to guess and try different things.

(4.1) Overall, the R&D work undertaken represented an advance relative to the baseline knowledge and capabilities that existed in the public domain. Consequently, the R&D efforts align with Paragraphs 6 and 20 of the DSIT Guidelines.


# Existing technological and scientific solutions and knowledge *

(1.1) For context, The Developing Leaders Partnership Limited specialises in the engineering of high-fidelity, data-driven systems that translate qualitative ‘leadership’ metrics into actionable organisational development strategies. 

(2.1) During their prior claim period (January 2024 to December 2024), their team of competent professionals developed a successor framework, RetortJS. The implementation of the framework utilised a JavaScript- and TypeScript-centric development pattern to manage reporting workflows. 

(2.2) At the start of the current claim period (January 2025), the team re-evaluated the successor branch and found it to be technologically inferior for dynamic, data-first operations and unsuitable for non-technical users to iterate upon workflows effectively. Consequently, the team discontinued RetortJS for this workstream and reverted to developing the predecessor in-house architecture, referred to as ‘Scratchpad’.

(3.1) Prior to undertaking the R&D activities described in the ‘overcome’ section of this technical report, the team reviewed the knowledge and capabilities in the public domain. During this assessment, the team found that whilst existing data-to-text systems could transform structured datasets into narrative summaries utilising predefined templates or single-pass natural language generation models, they were fundamentally incapable of performing multi-step analytical reasoning on qualitative inputs or articulating intermediate reasoning stages for human audit.

(3.2) Conversely, the team evaluated ‘off-the-shelf’ LLM-based systems and agent-oriented frameworks which prioritised speed and autonomy over conservative, explanatory analysis. From a software engineering perspective, these platforms lacked established mechanisms to enforce global consistency across deep, multi-stage pipelines. More specifically, the platforms did not address the challenge of maintaining consistency in non-deterministic distributed reasoning, a problem analogous to the Consistency, Availability, and Partition Tolerance (CAP) Theorem in distributed databases, where local outputs might be plausible but globally contradictory. In particular, the team found no apparent/existing prompt-chaining techniques with a proven implementation path to anchor regressions or maintain end-to-end traceability from conclusions back to source inputs.

(3.3) Evaluation of emerging research and prototype techniques, such as Automatic Prompt Engineer (APE) and Optimised Prompt Result (OPRO), revealed that they generally assumed clearer objective functions than are typical in deep analytical settings. While tools like LangSmith and OpenAI Evals provided reference approaches for dataset-based evaluation and rubric-style grading, they did not offer a dependable, generally repeatable way to create and iterate deep, highly interdependent prompt chains. 

(3.4) At the start of 2025, the field understood that LLMs behaved as stochastic, non-deterministic ‘black-box’ systems where the same input could produce meaningfully different outputs across runs. Consequently, the development of long chains with many interacting quality requirements remained regression-prone and labour-intensive, with limited ability to control non-local regressions across the chain.

(4.1) Overall, because of these aforementioned technical limitations, the team of competent professionals found that these systems and agent frameworks could not be readily adapted or modified to meet their set of targeted outcomes. Additionally, the knowledge and capabilities in the public domain did not provide sufficient technical detail or know-how to readily deduce how to achieve the technological advance. As such, the team elected to continue their programme of R&D.

(5.1) The technical team was led by the competent professional, Daniel Huggins, who was The Developing Leaders Partnership Limited’s Co-Founder and Head of Research. Daniel holds a Master’s degree in Maths from Durham University and has 14 years of experience working in the field.


# Technological and scientific uncertainties *

(1.1) During the claim period (January 2025 to December 2025), technological uncertainty lay in how to create a new audible and stable reporting system capable of multi-step qualitative reasoning. Within this overarching uncertainty existed a number of smaller, more specific uncertainties. The following section provides examples of these.

(2.1) Specific technological uncertainty revolved around whether deep analytical prompt chains comprising 10 or more interdependent steps could achieve sufficient dependability in a production environment. In particular, it was unknown whether a stable operating point existed for chains of this depth, as reliability was expected to degrade with each successive step; for example, a 99 % success rate per step yields only approximately 90 % reliability over 10 steps. As LLMs operate as stochastic, nondeterministic ‘black-box’ systems, a competent professional working in the field at the time could not confidently predict whether acceptable end-to-end failure rates of less than 5 % were achievable.

(2.2) Additional uncertainty was identified in the instruction load sensitivity of these models, where adding constraints or additional instructions often degraded performance across multiple dimensions rather than improving it. Specifically, it was unknown how to balance complex analytical instructions with the inherent context window limitations of the models without causing a degradation in the reasoning quality of the downstream steps; this presented a significant technical barrier, as the team could not anticipate ahead of time which seemingly minor edits to a prompt would produce unexpected degradations in output quality.

(3.1) Moreover, they were uncertain whether deep analytical prompt chains could be changed at a single-step level without small upstream variations subtly altering an intermediate artefact, which in turn altered the input distribution for downstream steps and cascaded into materially different outcomes; this was considered a major technological challenge because stochastic model variation and complex chain interaction effects can mask or mimic genuine changes, making regressions difficult to detect and attribute within nondeterministic pipelines.

(4.1) Further technological uncertainty lay around whether LLMs could be utilised as dependable meta-tools to diagnose failure patterns and propose coherent revisions across a complex chain. More specifically, there were concerns around whether an LLM-as-judge could reliably guide multi-dimensional qualitative constraints (such as correctness, analysis quality, and structure) rather than merely executing explicitly specified edits. At the start of the claim period (January 2025), there was no generally accepted method showing that automated diagnosis and revision could be relied upon to guide prompt-chain iteration in this difficulty class without introducing plausible but technically incorrect judgments.

(5.1) Furthermore, technological uncertainty existed in how to achieve end-to-end traceability and behavioural stability. To expand on this, the team questioned whether trace links could be maintained from conclusions back through multiple intermediate transformations to the source inputs without incurring undue fragility. As qualitative outputs were susceptible to subtle variations stemming from probabilistic token sampling and hardware-level precision differences, maintaining the integrity of decision-support outputs while extending system capabilities remained a core unknown.

(6.1) Work done to overcome these technological uncertainties is described in the next section.


# R&D activities to resolve technical and scientific challenges *

(1.1) In an effort to overcome the aforementioned technological uncertainties encountered during the claim period (January 2025 to December 2025), the team of competent professionals elected to undertake a programme of R&D comprising technological planning, iterative design and testing, and prototyping. Given the size and scope of the R&D efforts, the following highlights examples of work undertaken.

(1.2) To note, at the beginning of the claim period (January 2025), the team decided to discontinue the RetortJS successor branch and continue development on the predecessor architecture lineage, known as Scratchpad, because it provided stronger support for data-centric workflow development and non-technical iteration. 

(2.1) To address technological uncertainty inherent in achieving adequate grounding and substantiation within deep prompt chains, the team developed and implemented an intermediate, structured representation, the Issue Register; this register functioned as a state-management layer that documented prioritised analytical issues, cross-referenced with specific input data and supporting artefacts. By adopting this approach, the team successfully decoupled analytical reasoning from narrative prose generation; this required issue identification and prioritisation to occur as explicit intermediate artefacts, with narrative assembly deferred until these reasoning tasks had been completed and verified against source evidence.

(2.2) To test the feasibility of dependable deep analytical chains at the required depth, the team built a pilot prompt chain on OpenAI-family models focused on data interpretation, analysis, and prose rendering. The team initially observed an end-to-end success rate of approximately 2 % and undertook a systematic programme of iteration toward a 95 % success threshold; this involved testing prompt-step variations, comparing output batches, and analysing recurring failures. Additionally, the team maintained a database of prompt changes and observed effects across quality dimensions to guide iteration and manage instruction load by redistributing requirements across steps.

(3.1) To address uncertainty about non-local downstream regressions, the team initially changed prompt-step edits to preserve the shape of intermediate artefacts. However, they found that as the 95 % success threshold was approached, this method became impractical for deep chains, as only very small changes reliably avoided downstream effects. Consequently, the team shifted to a power-through systematic approach: treating downstream regressions as expected while utilising Gemini’s separate analysis capabilities to rapidly attribute breakage to specific step-to-step interface changes. Using this diagnostic knowledge, the team was able to rapidly draft and re-test compensating downstream prompt edits to restore system integrity.

(3.2) The team also integrated evaluation and improvement stages at multiple points throughout the Scratchpad pipeline to address the uncertainty of maintaining cross-sectional consistency. These stages were inserted after key intermediate reasoning outputs and following final prose assembly to test whether local conclusions remained consistent with upstream assumptions. Subsequent experiments comparing pre-assembly and post-assembly evaluation placement revealed that earlier placement reduced contradiction rates, particularly when subject attributes were inconsistently characterised across sections, such as discrepancies between a curriculum vitae and a psychological assessment.

(4.1) To address uncertainty about the use of LLMs as dependable meta-tools, the team trialled rubric-style LLM-as-judge prompts in a batch-evaluation harness to compare chain versions. Initial attempts at single-pass judging proved difficult to align with multidimensional, partly tacit criteria such as correctness and analysis quality. Furthermore, because outputs and judgments were non-deterministic, reducing variance required repeated runs, which became computationally and temporally impractical. To resolve this, the team iterated an interactive, human-in-the-loop conversational meta-prompt workflow using Gemini and later O3 models. By supplying concrete failure examples and constraints, the team utilised the LLM to analyse step outputs and pattern-find across output batches to guide the generation of targeted edit candidates.

(5.1) To overcome uncertainty concerning replayability and regression anchoring, both workflow and prompt versions were pinned for each build; this involved tying each testable configuration to an explicit versioned bundle (comprising workflow graph definitions, prompt text, and model settings) to ensure that qualitative inputs could be re-executed against an identical configuration. Fixed sample sets were run in batches to measure behavioural shifts. For mature pinned workflows, the team established a working tolerance of approximately 5 % maximum divergence, with several review batches achieving zero material failures across approximately 30 repeated executions.

(6.1) Overall, whilst a number of technological uncertainties have been overcome in this claim period, several uncertainties remained unresolved; for example, there was still uncertainty around achieving dependable multidimensional regression detection at deep-chain depth under stochastic model behaviour. As such, the R&D project was ongoing into the next claim period (beyond December 2025).
